# -*- coding: utf-8 -*-
"""巨量期末v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v7SsYQWVmWbzhtwn6obLrNfaU4H3H9hv
"""

# Deep Embedded Clustering (DEC) - Full Version

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
from torch.optim.lr_scheduler import ReduceLROnPlateau

import pandas as pd
import numpy as np
import os
import random

!lscpu
!free -h

def set_seed(seed):
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['OMP_NUM_THREADS'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    # PyTorch相關設置
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # 多 GPU 時使用
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False # 重要！關閉benchmark以確保確定性

    # 設置NumPy和PyTorch的隨機數生成器
    np_rng = np.random.RandomState(seed)
    torch_rng = torch.Generator()
    torch_rng.manual_seed(seed)

    return np_rng, torch_rng


# 呼叫
SEED = 20
np_rng, torch_rng = set_seed(SEED)


# 設定 numpy
np.random.seed(SEED)

# 設定 torch（CPU）
torch.manual_seed(SEED)
torch.use_deterministic_algorithms(True)

# 設定 DataLoader
g = torch.Generator()
g.manual_seed(SEED)

# ========== 1. 加載和預處理數據 ==========
# Public dataset
df_public = pd.read_csv('public_data.csv')  # 4 features
features_public = ['1', '2', '3', '4']
scaler_public = StandardScaler()
X_public_scaled = scaler_public.fit_transform(df_public[features_public]) #標準化
X_public_tensor = torch.tensor(X_public_scaled, dtype=torch.float32) #轉成tensor

# Private dataset
df_private = pd.read_csv('private_data.csv')  # 6 features
features_private = ['1', '2', '3', '4', '5', '6']  # 調整為實際特徵名
scaler_private = StandardScaler()
X_private_scaled = scaler_private.fit_transform(df_private[features_private])
X_private_tensor = torch.tensor(X_private_scaled, dtype=torch.float32)

# 創建數據加載器
public_dataset = TensorDataset(X_public_tensor)
private_dataset = TensorDataset(X_private_tensor)
public_loader = DataLoader(public_dataset, batch_size=512, shuffle=True, generator=torch_rng)
private_loader = DataLoader(private_dataset, batch_size=1024, shuffle=True, generator=torch_rng)

# ========== 2. 改進雙通道自動編碼器 ==========
class ImprovedDualChannelAutoencoder(nn.Module):
    def __init__(self, public_dim=4, private_dim=6, latent_dim=16):  # 增加潛在空間維度
        super().__init__()
        self.public_dim = public_dim
        self.private_dim = private_dim

        # Public 通道的編碼器 (加深網絡)
        self.public_encoder = nn.Sequential(
            nn.Linear(public_dim, 32),
            nn.BatchNorm1d(32),  # 新增批標準化
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),    # 新增Dropout
            nn.Linear(32, 64),
            nn.BatchNorm1d(64),
            nn.LeakyReLU(0.2),
            nn.Linear(64, latent_dim)
        )

        # Private 通道的編碼器 (加深網絡)
        self.private_encoder = nn.Sequential(
            nn.Linear(private_dim, 64),
            nn.BatchNorm1d(64),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),
            nn.Linear(64, 128),
            nn.BatchNorm1d(128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, latent_dim)
        )


        self.decoder_public = nn.Sequential(
            nn.Linear(latent_dim, 64),
            nn.BatchNorm1d(64),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.LeakyReLU(0.2),
            nn.Linear(32, public_dim)
        )

        self.decoder_private = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.BatchNorm1d(128),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.LeakyReLU(0.2),
            nn.Linear(64, private_dim)
        )

    def forward(self, x_public, x_private):
        # 處理public輸入
        if x_public is not None and x_public.size(0) > 0:
            z_public = self.public_encoder(x_public)
            recon_public = self.decoder_public(z_public)
        else:
            z_public = torch.tensor([])
            recon_public = torch.tensor([])

        # 處理private輸入
        if x_private is not None and x_private.size(0) > 0:
            z_private = self.private_encoder(x_private)
            recon_private = self.decoder_private(z_private)
        else:
            z_private = torch.tensor([])
            recon_private = torch.tensor([])

        return recon_public, recon_private, z_public, z_private

# 權重初始化
def init_weights(m):
    if isinstance(m, nn.Linear):
        torch.nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')
        if m.bias is not None:
            m.bias.data.fill_(0.01)

# ========== 3. 改進預訓練過程 ==========
model = ImprovedDualChannelAutoencoder(latent_dim=16)  # 使用改進模型
model.apply(init_weights)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)  # 增加權重衰減
scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)  # 新增學習率調度
loss_fn = nn.MSELoss()

for epoch in range(200):  # 增加訓練輪數
    model.train()
    total_loss = 0
    min_batches = min(len(public_loader), len(private_loader))
    public_iter = iter(public_loader)
    private_iter = iter(private_loader)

    for _ in range(min_batches):
        x_public = next(public_iter)[0]
        x_private = next(private_iter)[0]

        recon_public, recon_private, _, _ = model(x_public, x_private)

        loss_public = loss_fn(recon_public, x_public) if recon_public.numel() > 0 else 0
        loss_private = loss_fn(recon_private, x_private) if recon_private.numel() > 0 else 0
        loss = loss_public + loss_private

        optimizer.zero_grad()
        if loss > 0:
            loss.backward()
            # 新增梯度裁剪
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
            optimizer.step()

        total_loss += loss.item() if loss > 0 else 0

    avg_loss = total_loss / min_batches if min_batches > 0 else 0
    scheduler.step(avg_loss)  # 更新學習率
    print(f"Epoch {epoch+1}, Pretrain Loss: {avg_loss:.6f}")

# ========== 4. 獲取潛在特徵 ==========
with torch.no_grad():
    model.eval()
    _, _, z_public, _ = model(X_public_tensor, None)
    _, _, _, z_private = model(None, X_private_tensor)

# ========== 5. 改進分群中心初始化 ==========
# 使用KMeans++初始化並增加迭代次數
kmeans_public = KMeans(n_clusters=15, init='k-means++', n_init=50, max_iter=500, random_state=SEED)
kmeans_public.fit(z_public.numpy())
cluster_centers_public = torch.tensor(kmeans_public.cluster_centers_, dtype=torch.float32)

kmeans_private = KMeans(n_clusters=23, init='k-means++', n_init=50, max_iter=500, random_state=SEED)
kmeans_private.fit(z_private.numpy())
cluster_centers_private = torch.tensor(kmeans_private.cluster_centers_, dtype=torch.float32)

# ========== 6. 分群層 ==========
class DualClusteringLayer(nn.Module):
    def __init__(self, centers_public, centers_private):
        super().__init__()
        self.centers_public = nn.Parameter(centers_public)
        self.centers_private = nn.Parameter(centers_private)

    def forward(self, z, data_type):
        if data_type == 'public':
            dist = torch.cdist(z, self.centers_public)
        else:
            dist = torch.cdist(z, self.centers_private)

        # 使用更穩定的軟分配
        q = 1.0 / (1.0 + dist**2)
        q = q ** 2  # 平方使分配更明確
        q = q / q.sum(dim=1, keepdim=True)
        return q

cluster_layer = DualClusteringLayer(cluster_centers_public, cluster_centers_private)

# ========== 7. 改進目標分佈函數 ==========
def target_distribution(q):
    # 使用更穩定的目標分佈計算
    weight = q ** 2 / torch.sum(q, dim=0)
    return (weight.t() / torch.sum(weight, dim=1)).t()

# ========== 8. 改進聯合訓練 ==========
optimizer_joint = torch.optim.Adam(
    list(model.parameters()) + list(cluster_layer.parameters()),
    lr=1e-4,
    weight_decay=1e-5
)
scheduler_joint = ReduceLROnPlateau(optimizer_joint, 'min', patience=3, factor=0.5)

# 創建全數據集加載器
full_public_loader = DataLoader(public_dataset, batch_size=512, shuffle=False)
full_private_loader = DataLoader(private_dataset, batch_size=1024, shuffle=False)

# 新增：提前計算目標分佈的緩存
def compute_target_distribution(model, cluster_layer, loader, data_type):
    model.eval()
    cluster_layer.eval()
    q_all = []

    with torch.no_grad():
        for batch in loader:
            x = batch[0]
            if data_type == 'public':
                _, _, z, _ = model(x, None)
                q = cluster_layer(z, 'public')
            else:
                _, _, _, z = model(None, x)
                q = cluster_layer(z, 'private')
            q_all.append(q)

        q_all = torch.cat(q_all)
        p_all = target_distribution(q_all)
    return p_all

# 預先計算目標分佈
p_public_target = compute_target_distribution(model, cluster_layer, full_public_loader, 'public')
p_private_target = compute_target_distribution(model, cluster_layer, full_private_loader, 'private')

# 訓練循環
for epoch in range(150):  # 增加聯合訓練輪數
    model.train()
    cluster_layer.train()
    total_kl_loss = 0
    min_batches = min(len(public_loader), len(private_loader))

    # 新增：每10輪更新一次目標分佈
    if epoch % 10 == 0:
        p_public_target = compute_target_distribution(model, cluster_layer, full_public_loader, 'public')
        p_private_target = compute_target_distribution(model, cluster_layer, full_private_loader, 'private')

    public_iter = iter(DataLoader(TensorDataset(X_public_tensor, p_public_target),
                                batch_size=512, shuffle=True, generator=torch_rng))
    private_iter = iter(DataLoader(TensorDataset(X_private_tensor, p_private_target),
                                batch_size=1024, shuffle=True, generator=torch_rng))

    for _ in range(min_batches):
        # Public 數據
        x_public, p_public = next(public_iter)
        _, _, z_public, _ = model(x_public, None)
        q_public = cluster_layer(z_public, 'public')
        kl_loss_public = torch.nn.functional.kl_div(q_public.log(), p_public, reduction='batchmean')

        # Private 數據
        x_private, p_private = next(private_iter)
        _, _, _, z_private = model(None, x_private)
        q_private = cluster_layer(z_private, 'private')
        kl_loss_private = torch.nn.functional.kl_div(q_private.log(), p_private, reduction='batchmean')

        # 組合損失 + 正則化
        kl_loss = kl_loss_public + kl_loss_private
        total_kl_loss += kl_loss.item()

        optimizer_joint.zero_grad()
        kl_loss.backward()
        torch.nn.utils.clip_grad_norm_(list(model.parameters()) + list(cluster_layer.parameters()), 5.0)
        optimizer_joint.step()

    avg_kl_loss = total_kl_loss / min_batches
    scheduler_joint.step(avg_kl_loss)
    print(f"Epoch {epoch+1}, Joint KL Loss: {avg_kl_loss:.6f}, LR: {optimizer_joint.param_groups[0]['lr']:.2e}")

# ========== 9. 獲取最終分群結果 ==========
with torch.no_grad():
    model.eval()
    cluster_layer.eval()

    # Public 分群
    _, _, z_public, _ = model(X_public_tensor, None)
    q_public = cluster_layer(z_public, 'public')
    public_clusters = q_public.argmax(dim=1).numpy()

    # Private 分群
    _, _, _, z_private = model(None, X_private_tensor)
    q_private = cluster_layer(z_private, 'private')
    private_clusters = q_private.argmax(dim=1).numpy()

# 新增：計算分群質心距離作為內部評估
def calculate_cluster_quality(z, clusters):
    centroids = []
    for i in np.unique(clusters):
        centroids.append(z[clusters == i].mean(axis=0))
    centroids = np.array(centroids)

    # 計算樣本到所屬質心的平均距離
    distances = []
    for i in range(len(z)):
        distances.append(np.linalg.norm(z[i] - centroids[clusters[i]]))
    return np.mean(distances)

# 輸出分群品質
public_quality = calculate_cluster_quality(z_public.numpy(), public_clusters)
private_quality = calculate_cluster_quality(z_private.numpy(), private_clusters)
print(f"Public cluster quality: {public_quality:.4f}")
print(f"Private cluster quality: {private_quality:.4f}")

# 輸出分群結果
print("Public clusters:", np.unique(public_clusters, return_counts=True))
print("Private clusters:", np.unique(private_clusters, return_counts=True))

# ========== 10. PCA可視化 ==========
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA

# 確保這些變數已經定義（從第二段程式碼獲取）
# z_public, z_private, public_clusters, private_clusters 必須已存在

# 轉換為numpy array（如果尚未轉換）
z_public_np = z_public.numpy() if torch.is_tensor(z_public) else z_public
z_private_np = z_private.numpy() if torch.is_tensor(z_private) else z_private
public_clusters_np = public_clusters.numpy() if torch.is_tensor(public_clusters) else public_clusters
private_clusters_np = private_clusters.numpy() if torch.is_tensor(private_clusters) else private_clusters

# PCA降維
def perform_pca_visualization(data, clusters, title, n_clusters):
    pca = PCA(n_components=3)
    data_3d = pca.fit_transform(data)

    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    # 為每個聚類設置顏色
    colors = plt.cm.get_cmap('tab20', n_clusters)

    for i in range(n_clusters):
        mask = clusters == i
        ax.scatter(
            data_3d[mask, 0],
            data_3d[mask, 1],
            data_3d[mask, 2],
            s=30,
            color=colors(i),
            label=f'Cluster {i}',
            alpha=0.7
        )

    ax.set_xlabel('PCA Component 1')
    ax.set_ylabel('PCA Component 2')
    ax.set_zlabel('PCA Component 3')
    plt.title(title)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.show()

    # 輸出解釋方差比
    print(f"{title} PCA解釋方差比:", pca.explained_variance_ratio_)
    print(f"累計解釋方差:", np.cumsum(pca.explained_variance_ratio_))

# Public數據可視化
perform_pca_visualization(
    z_public_np,
    public_clusters_np,
    'Public Dataset Clusters (15 clusters)',
    15
)

# Private數據可視化
perform_pca_visualization(
    z_private_np,
    private_clusters_np,
    'Private Dataset Clusters (23 clusters)',
    23
)

public_submission = pd.DataFrame({'id': range(1, df_public.shape[0]+1)})
public_submission['label']=public_clusters
public_submission.to_csv('public_submission.csv', index=False)

#改
# ========== 10. 儲存完整模型和相關組件 ==========
import os
import joblib

# 創建儲存目錄
model_dir = "saved_models"
os.makedirs(model_dir, exist_ok=True)

# 儲存完整模型狀態 (包含雙通道自動編碼器和分群層)
torch.save({
    'model_state_dict': model.state_dict(),
    'cluster_layer_state_dict': cluster_layer.state_dict(),
    'public_scaler': scaler_public,
    'private_scaler': scaler_private,
    'kmeans_public': kmeans_public,
    'kmeans_private': kmeans_private,
    'public_dim': model.public_dim,
    'private_dim': model.private_dim,
    'latent_dim': model.latent_dim,
    'n_clusters_public': 15,
    'n_clusters_private': 23
}, os.path.join(model_dir, "dual_channel_cluster_model.pth"))

print("完整模型已儲存")

# 額外儲存標準化器 (方便後續單獨使用)
joblib.dump(scaler_public, os.path.join(model_dir, "public_scaler.joblib"))
joblib.dump(scaler_private, os.path.join(model_dir, "private_scaler.joblib"))

# 儲存分群結果 (可選)
np.save(os.path.join(model_dir, "public_clusters.npy"), public_clusters)
np.save(os.path.join(model_dir, "private_clusters.npy"), private_clusters)

public_submission['label'].value_counts()