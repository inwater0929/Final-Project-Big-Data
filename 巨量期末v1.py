# -*- coding: utf-8 -*-
"""巨量期末v1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TCwzcvQs47lRd6juqveYD4oqNx2ZKKTF
"""

# Deep Embedded Clustering (DEC) - Full Version

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import pandas as pd
import numpy as np
import os
import random

def set_seed(seed):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # 多 GPU 時使用
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# 呼叫
SEED=40
set_seed(SEED)


# 設定 numpy
np.random.seed(SEED)

# 設定 torch（CPU）
torch.manual_seed(SEED)
torch.use_deterministic_algorithms(True)

# 設定 DataLoader
g = torch.Generator()
g.manual_seed(SEED)

# ========== 1. Load and preprocess data ==========
df = pd.read_csv('public_data.csv')  # Replace with your CSV file
features = ['1', '2', '3', '4']  # Adjust feature names
scaler = StandardScaler() #標準化
X_scaled = scaler.fit_transform(df[features])
X_tensor = torch.tensor(X_scaled, dtype=torch.float32)

dataset = TensorDataset(X_tensor)
dataloader = DataLoader(dataset, batch_size=512, shuffle=True)

# ========== 2. Define Autoencoder ==========
class Autoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(4, 16),
            nn.ReLU(),
            nn.Linear(16, 32),
            nn.ReLU(),
            nn.Linear(32, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 8)  # Latent space
        )
        self.decoder = nn.Sequential(
            nn.Linear(8, 8),
            nn.ReLU(),
            nn.Linear(8, 16),
            nn.ReLU(),
            nn.Linear(16, 4)
        )

    def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon, z

model = Autoencoder()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
loss_fn = nn.MSELoss()
# ReduceLROnPlateau 是 PyTorch 的一種 自動調整學習率的 scheduler
# 當監控的指標（如 validation loss）在一定訓練輪數（patience）內沒有改善時，
# 就會自動降低學習率，通常搭配 optimizer.step() 使用。
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',         # 監控的指標越小越好（例如loss）
    factor=0.1,         # 每次調整學習率的倍數（新lr = lr * factor）
    patience=5,         # 如果N個epoch內沒改善，就調整學習率
    verbose=True        # 顯示每次學習率更新的log
)
# ========== 3. Pretrain Autoencoder ==========
for epoch in range(300):
    total_loss = 0
    for batch in dataloader:
        x_batch, = batch
        x_recon, _ = model(x_batch)
        loss = loss_fn(x_recon, x_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}, Pretrain Loss: {total_loss:.4f}")

# ========== 4. Get latent features ==========
with torch.no_grad():
    _, Z = model(X_tensor)

# ========== 5. Initialize cluster centers using KMeans ==========
kmeans = KMeans(n_clusters=15, n_init=20)
cluster_centers = torch.tensor(kmeans.fit(Z.numpy()).cluster_centers_, dtype=torch.float32)

# ========== 6. Clustering layer ==========
class ClusteringLayer(nn.Module):
    def __init__(self, cluster_centers):
        super().__init__()
        self.centers = nn.Parameter(cluster_centers)

    def forward(self, z):
        q = 1.0 / (1.0 + torch.sum((z.unsqueeze(1) - self.centers) ** 2, dim=2))
        q = q / torch.sum(q, dim=1, keepdim=True)
        return q

cluster_layer = ClusteringLayer(cluster_centers)

# ========== 7. Target distribution function ==========
def target_distribution(q):
    weight = q ** 2 / q.sum(0)
    return (weight.T / weight.sum(1)).T

# ========== 8. Joint training (DEC phase) ==========
all_dataset = DataLoader(dataset, batch_size=512, shuffle=False)
optimizer_joint = torch.optim.Adam(list(model.parameters()) + list(cluster_layer.parameters()), lr=1e-3)

for epoch in range(100):
    q_all = []
    z_all = []
    model.eval()
    cluster_layer.eval()
    with torch.no_grad():
        for batch in all_dataset:
            x_batch, = batch
            _, z = model(x_batch)
            q = cluster_layer(z)
            q_all.append(q)
            z_all.append(z)
    q_all = torch.cat(q_all)
    z_all = torch.cat(z_all)
    p_all = target_distribution(q_all)

    # Shuffle training
    model.train()
    cluster_layer.train()
    idx = torch.randperm(X_tensor.shape[0])
    X_shuffled = X_tensor[idx]
    p_shuffled = p_all[idx]
    train_dataset = DataLoader(TensorDataset(X_shuffled, p_shuffled), batch_size=512)

    total_kl_loss = 0
    for batch in train_dataset:
        x_batch, p_batch = batch
        _, z = model(x_batch)
        q = cluster_layer(z)
        kl_loss = torch.nn.functional.kl_div(q.log(), p_batch, reduction='batchmean')
        optimizer_joint.zero_grad()
        kl_loss.backward()
        optimizer_joint.step()
        total_kl_loss += kl_loss.item()
    print(f"Epoch {epoch+1}, DEC KL Loss: {total_kl_loss:.4f}")

# ✅ 1. 取出每筆資料的分群機率分布與預測分群結果

# 切換為推論模式
model.eval()
cluster_layer.eval()

with torch.no_grad():
    _, z_all = model(X_tensor)              # 編碼後的潛在表示
    q_all = cluster_layer(z_all)            # 分群機率分布 (N, 15)
    pred_labels = torch.argmax(q_all, dim=1)  # 分群結果 (每筆資料所屬的類別)

# 可以存成 DataFrame
df['label'] = pred_labels.numpy()
print(df[['label']].value_counts())

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# PCA 降到 2 維
pca = PCA(n_components=3)
z_pca = pca.fit_transform(z_all.numpy())

# 畫圖
import plotly.express as px
import pandas as pd

z_df = pd.DataFrame(z_pca, columns=['PC1', 'PC2', 'PC3'])
z_df['cluster'] = pred_labels.numpy()

fig = px.scatter_3d(
    z_df,
    x='PC1', y='PC2', z='PC3',
    color='cluster',
    title='DEC Clustered Latent Space (PCA 3D)',
    size_max=5  # 最大點大小限制
)
fig.update_traces(marker=dict(size=1))  # 固定點大小為3
fig.show()

# 儲存整體模型
torch.save({
    'autoencoder': model.state_dict(),
    'cluster_layer': cluster_layer.state_dict(),
    'scaler': scaler,
}, 'dec_model.pth')

# 儲存潛在變數與分群結果
z_df = pd.DataFrame(z_all.numpy(), columns=['z1', 'z2', 'z3','z4','z5','z6','z7','z8'])
z_df['label'] = pred_labels.numpy()
z_df.to_csv('latent_clusters.csv', index=False)

df[['id','label']]
df[['id','label']].to_csv('public_submission.csv', index=False)

df

z_df